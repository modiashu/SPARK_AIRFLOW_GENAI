# ============================================================================
# Airflow Configuration Overrides
# ============================================================================
# This file contains minimal Airflow configuration overrides.
# Place this in your AIRFLOW_HOME directory or use it as a reference.
# 
# For local development, many defaults are suitable. Key configurations:
# - Use SQLite for simplicity (production should use PostgreSQL/MySQL)
# - Use SequentialExecutor for simple sequential execution (compatible with SQLite)
# - Disable example DAGs to reduce clutter
# ============================================================================

[core]
# The folder where your airflow pipelines live
dags_folder = ${AIRFLOW_HOME}/dags

# Whether to load the DAG examples that ship with Airflow
load_examples = False

# The executor class that airflow should use
# SequentialExecutor: Tasks run one at a time (compatible with SQLite)
# For parallel execution, use LocalExecutor with PostgreSQL/MySQL
executor = SequentialExecutor

# The amount of parallelism as a setting to the executor
# Note: SequentialExecutor ignores this setting
parallelism = 1

# The number of task instances allowed to run concurrently per DAG
# Note: SequentialExecutor runs tasks sequentially
dag_concurrency = 1

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 1

[database]
# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = sqlite:///${AIRFLOW_HOME}/airflow.db

[logging]
# The folder where airflow should store its log files
base_log_folder = ${AIRFLOW_HOME}/logs

# Logging level
logging_level = INFO

# Log format for when logs are written to files
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

[webserver]
# The port on which to run the web server
web_server_port = 8080

# Secret key to save session state
secret_key = temporary_key_for_local_development_only

# Number of seconds the webserver waits before killing gunicorn master
web_server_master_timeout = 120

# Number of workers to run the webserver on
workers = 2

[scheduler]
# The scheduler constantly tries to trigger new tasks
# This defines how often the scheduler checks for new tasks
scheduler_heartbeat_sec = 5

# Number of times to try to schedule a DAG run before giving up
num_runs = -1

[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.basic_auth

[operators]
# The default owner assigned to each new operator, unless provided explicitly or passed via default_args
default_owner = Airflow
