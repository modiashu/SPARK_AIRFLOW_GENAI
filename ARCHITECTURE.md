# Architecture and Workflow Diagrams

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Local Development Machine                   â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    Apache Airflow (2.9.3)                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚   â”‚
â”‚  â”‚  â”‚  Webserver   â”‚  â”‚  Scheduler   â”‚  â”‚   Database   â”‚       â”‚   â”‚
â”‚  â”‚  â”‚  (Port 8080) â”‚  â”‚  (Triggers   â”‚  â”‚   (SQLite)   â”‚       â”‚   â”‚
â”‚  â”‚  â”‚              â”‚  â”‚   DAGs)      â”‚  â”‚              â”‚       â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                       â”‚
â”‚                              â”‚ Triggers                              â”‚
â”‚                              â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Airflow DAG: hello_world_spark_airflow         â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚   â”‚
â”‚  â”‚   â”‚ Start Task   â”‚  (PythonOperator)                        â”‚   â”‚
â”‚  â”‚   â”‚ Initialize   â”‚  - Log start time                        â”‚   â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Set up context                        â”‚   â”‚
â”‚  â”‚          â”‚                                                   â”‚   â”‚
â”‚  â”‚          â–¼                                                   â”‚   â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚   â”‚
â”‚  â”‚   â”‚  Validate    â”‚  (PythonOperator)                        â”‚   â”‚
â”‚  â”‚   â”‚ Environment  â”‚  - Check input files exist               â”‚   â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Verify Spark job script              â”‚   â”‚
â”‚  â”‚          â”‚          - Ensure output dir is writable         â”‚   â”‚
â”‚  â”‚          â–¼                                                   â”‚   â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚   â”‚
â”‚  â”‚   â”‚  Run Spark   â”‚  (BashOperator)                          â”‚   â”‚
â”‚  â”‚   â”‚    Job       â”‚  - Execute hello_world_spark.py          â”‚   â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Process data                          â”‚   â”‚
â”‚  â”‚          â”‚                                                   â”‚   â”‚
â”‚  â”‚          â–¼                                                   â”‚   â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚   â”‚
â”‚  â”‚   â”‚  Validate    â”‚  (PythonOperator)                        â”‚   â”‚
â”‚  â”‚   â”‚   Output     â”‚  - Check output files created            â”‚   â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  - Verify data integrity                â”‚   â”‚
â”‚  â”‚          â”‚                                                   â”‚   â”‚
â”‚  â”‚          â–¼                                                   â”‚   â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚   â”‚
â”‚  â”‚   â”‚  End Task    â”‚  (PythonOperator)                        â”‚   â”‚
â”‚  â”‚   â”‚  Finalize    â”‚  - Log completion                        â”‚   â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Cleanup                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                       â”‚
â”‚                              â”‚ Invokes                               â”‚
â”‚                              â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                Apache Spark (3.5.1) - Local Mode             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚              Spark Job: hello_world_spark.py         â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                                                        â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Step 1: Read Data                                    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ spark.read.csv()               â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - Read sample_sales_data.csv   â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - Infer schema                 â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - 15 records                   â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚          â”‚                                            â”‚   â”‚   â”‚
â”‚  â”‚  â”‚          â–¼                                            â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Step 2: Transform Data                              â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ df.withColumn()                â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - Calculate total_amount       â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - Filter amount > 100          â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - Select columns               â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚          â”‚                                            â”‚   â”‚   â”‚
â”‚  â”‚  â”‚          â–¼                                            â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Step 3: Aggregate Data                              â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ DataFrame API:                 â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - groupBy + agg                â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚                                â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ Spark SQL:                     â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - createOrReplaceTempView      â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - spark.sql()                  â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚          â”‚                                            â”‚   â”‚   â”‚
â”‚  â”‚  â”‚          â–¼                                            â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Step 4: Write Output                                â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ df.write.csv()                 â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - transformed_data             â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - sales_by_category            â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - avg_by_region                â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ - top_customers                â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                       â”‚
â”‚                              â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      File System                              â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚  Input:  data/input/sample_sales_data.csv                    â”‚   â”‚
â”‚  â”‚  Output: data/output/                                        â”‚   â”‚
â”‚  â”‚          - transformed_data_YYYYMMDD_HHMMSS/                 â”‚   â”‚
â”‚  â”‚          - sales_by_category_YYYYMMDD_HHMMSS/                â”‚   â”‚
â”‚  â”‚          - avg_by_region_YYYYMMDD_HHMMSS/                    â”‚   â”‚
â”‚  â”‚          - top_customers_YYYYMMDD_HHMMSS/                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input     â”‚
â”‚   CSV File  â”‚  (15 transaction records)
â”‚  (Sales     â”‚  - transaction_id, customer_id
â”‚   Data)     â”‚  - product_name, category
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  - quantity, unit_price
       â”‚         - transaction_date, region
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Spark: Read CSV                             â”‚
â”‚  df = spark.read.csv(path, header=True)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Transformation 1: Add Calculated Column          â”‚
â”‚  df = df.withColumn("total_amount",                 â”‚
â”‚                     quantity * unit_price)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Transformation 2: Filter Data                    â”‚
â”‚  df = df.filter(col("total_amount") > 100)          â”‚
â”‚  (Keeps only high-value transactions)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                                     â”‚
                   â–¼                                     â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Aggregation Path 1:     â”‚         â”‚  Aggregation Path 2:     â”‚
   â”‚   DataFrame API           â”‚         â”‚  Spark SQL               â”‚
   â”‚                           â”‚         â”‚                          â”‚
   â”‚  sales_by_category =      â”‚         â”‚  CREATE TEMP VIEW sales  â”‚
   â”‚    df.groupBy("category") â”‚         â”‚                          â”‚
   â”‚      .agg(                â”‚         â”‚  SELECT region,          â”‚
   â”‚        sum("total"),      â”‚         â”‚    AVG(total_amount),    â”‚
   â”‚        count("*")         â”‚         â”‚    COUNT(*),             â”‚
   â”‚      )                    â”‚         â”‚    SUM(total_amount)     â”‚
   â”‚                           â”‚         â”‚  FROM sales              â”‚
   â”‚  Result:                  â”‚         â”‚  GROUP BY region         â”‚
   â”‚  - Electronics: $X        â”‚         â”‚                          â”‚
   â”‚  - Furniture: $Y          â”‚         â”‚  Result:                 â”‚
   â”‚  - Stationery: $Z         â”‚         â”‚  - North: avg $X         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  - South: avg $Y         â”‚
             â”‚                           â”‚  - East:  avg $Z         â”‚
             â”‚                           â”‚  - West:  avg $W         â”‚
             â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                    â”‚
             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
             â”‚                                    â”‚
             â–¼                                    â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Write Outputs (4 separate files)                  â”‚
   â”‚                                                      â”‚
   â”‚  1. transformed_data/     - Filtered transactions   â”‚
   â”‚  2. sales_by_category/    - Aggregated by category  â”‚
   â”‚  3. avg_by_region/        - Aggregated by region    â”‚
   â”‚  4. top_customers/        - Top 5 customers         â”‚
   â”‚                                                      â”‚
   â”‚  Format: CSV with headers                           â”‚
   â”‚  Mode: Overwrite                                    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Output Files    â”‚
           â”‚  (data/output/)  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Interaction Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    User     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ (1) Trigger DAG
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow Webserver  â”‚  (Port 8080)
â”‚  (Flask App)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ (2) Send trigger request
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Airflow Scheduler   â”‚
â”‚ (Background Process)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ (3) Schedule tasks
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Task Queue         â”‚
â”‚  (SQLite DB)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ (4) Execute tasks
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                      â”‚
       â–¼                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PythonOperator â”‚              â”‚   BashOperator       â”‚
â”‚  (Validation)   â”‚              â”‚   (Run Spark)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                 â”‚
          â”‚ (5) Check files                â”‚ (6) Execute command
          â”‚                                 â”‚
          â–¼                                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Pass   â”‚                  â”‚  python3 spark_    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚  jobs/hello_world_ â”‚
                                  â”‚  spark.py          â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â”‚ (7) Create session
                                            â”‚
                                            â–¼
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚  SparkSession      â”‚
                                  â”‚  (local[*] mode)   â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â”‚ (8) Process data
                                            â”‚
                                            â–¼
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚  Read â†’ Transform  â”‚
                                  â”‚  Aggregate â†’ Write â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â”‚ (9) Write files
                                            â”‚
                                            â–¼
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚  data/output/      â”‚
                                  â”‚  (CSV files)       â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Technology Stack Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   User Interface                      â”‚
â”‚  - Airflow Web UI (http://localhost:8080)            â”‚
â”‚  - Spark UI (http://localhost:4040)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Orchestration Layer                     â”‚
â”‚  - Apache Airflow 2.9.3                              â”‚
â”‚    â€¢ DAG definition and scheduling                   â”‚
â”‚    â€¢ Task execution and monitoring                   â”‚
â”‚    â€¢ Error handling and retries                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Processing Layer                         â”‚
â”‚  - Apache Spark 3.5.1                                â”‚
â”‚    â€¢ Distributed data processing                     â”‚
â”‚    â€¢ DataFrame API & Spark SQL                       â”‚
â”‚    â€¢ In-memory computation                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Application Layer                       â”‚
â”‚  - Python 3.9-3.11                                   â”‚
â”‚  - PySpark API                                       â”‚
â”‚  - Custom business logic                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Storage Layer                          â”‚
â”‚  - SQLite (Airflow metadata)                         â”‚
â”‚  - CSV files (Input/Output data)                     â”‚
â”‚  - Local file system                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Infrastructure Layer                       â”‚
â”‚  - Local Machine (macOS/Linux/Windows)               â”‚
â”‚  - JVM (Java Virtual Machine)                        â”‚
â”‚  - Python Virtual Environment (UV)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Deployment Model

```
Development Setup (Local Machine)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Environment: Local Development                      â”‚
â”‚  Executor: SequentialExecutor                       â”‚
â”‚  Database: SQLite                                    â”‚
â”‚  Spark Mode: local[*]                                â”‚
â”‚  Parallelism: 4                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Future Production Setup (Example)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Environment: Cloud (AWS/GCP/Azure)                  â”‚
â”‚  Executor: CeleryExecutor or KubernetesExecutor      â”‚
â”‚  Database: PostgreSQL/MySQL                          â”‚
â”‚  Spark Mode: YARN/Kubernetes/Standalone cluster      â”‚
â”‚  Parallelism: Configurable based on resources        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## File Structure Visualization

```
SPARK_AIRFLOW_GENAI/
â”‚
â”œâ”€â”€ ğŸ“˜ Documentation Files
â”‚   â”œâ”€â”€ README.md              (Main documentation)
â”‚   â”œâ”€â”€ QUICKSTART.md          (Quick reference)
â”‚   â”œâ”€â”€ GETTING_STARTED.md     (Setup checklist)
â”‚   â”œâ”€â”€ PROJECT_SUMMARY.md     (Project overview)
â”‚   â”œâ”€â”€ CONTRIBUTING.md        (Contribution guide)
â”‚   â”œâ”€â”€ CHANGELOG.md           (Version history)
â”‚   â””â”€â”€ LICENSE                (MIT License)
â”‚
â”œâ”€â”€ âš™ï¸ Configuration
â”‚   â”œâ”€â”€ pyproject.toml         (Python dependencies)
â”‚   â”œâ”€â”€ .env.example           (Environment template)
â”‚   â”œâ”€â”€ .gitignore             (Git ignore rules)
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ airflow.cfg        (Airflow settings)
â”‚       â”œâ”€â”€ spark.conf         (Spark settings)
â”‚       â””â”€â”€ log4j.properties   (Logging config)
â”‚
â”œâ”€â”€ ğŸ”§ Automation Scripts
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ setup.sh           (Complete setup)
â”‚       â”œâ”€â”€ start_airflow.sh   (Start services)
â”‚       â”œâ”€â”€ stop_airflow.sh    (Stop services)
â”‚       â””â”€â”€ run_spark_job.sh   (Run job directly)
â”‚
â”œâ”€â”€ ğŸŒŠ Airflow Components
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ hello_world_dag.py (Main DAG)
â”‚
â”œâ”€â”€ âš¡ Spark Components
â”‚   â””â”€â”€ spark_jobs/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ hello_world_spark.py (Data processing)
â”‚
â”œâ”€â”€ ğŸ“Š Data
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ input/
â”‚       â”‚   â””â”€â”€ sample_sales_data.csv
â”‚       â””â”€â”€ output/            (Generated files)
â”‚
â””â”€â”€ ğŸ§ª Testing
    â””â”€â”€ tests/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ conftest.py
        â”œâ”€â”€ test_spark_job.py  (12 tests)
        â””â”€â”€ test_dag.py        (10 tests)
```
